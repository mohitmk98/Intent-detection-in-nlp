{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Bidirectional, CuDNNLSTM, Dense, Embedding, Dropout, Input, CuDNNGRU, GRU, LSTM\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D, MaxPooling1D, GlobalAveragePooling1D, concatenate,AveragePooling1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint,ReduceLROnPlateau\n",
    "\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import pickle\n",
    "import re\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget http://nlp.stanford.edu/data/glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!unzip glove.840B.300d.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_index = dict()\n",
    "with open('glove.840B.300d.txt') as glove:\n",
    "  for line in glove:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    try:\n",
    "      coefs = np.asarray(values[1:], dtype='float32')\n",
    "      embeddings_index[word] = coefs\n",
    "    except:\n",
    "      print(word)\n",
    "   \n",
    "print('Found %s word vectors.' % len(embeddings_index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('data cab.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data2=pd.read_csv('data hotels.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data3=pd.read_csv('data weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data4=pd.read_csv('data flight.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sam1 = data.sample(frac=1).reset_index(drop=True)\n",
    "data_sam1=data_sam1.drop(['Unnamed: 2','Unnamed: 3'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sam2 = data2.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sam3=data3.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_sam4=data4.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datamix=data_sam2.append(data_sam1)\n",
    "datamix=datamix.append(data_sam3)\n",
    "datamix=datamix.append(data_sam4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=datamix.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dumPREV = pd.get_dummies(df[\"type\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embed_size = 300 # how big is each word vector\n",
    "\n",
    "maxlen = 30 # max number of words in a question to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = df[\"query\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(queries)\n",
    "queries = tokenizer.texts_to_sequences(queries)\n",
    "queries = pad_sequences(queries, maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.DataFrame(queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2=pd.DataFrame(df['type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = len(word_index)\n",
    "print(nb_words)\n",
    "print(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = df1.join(dumPREV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = dataset.iloc[:, :-4].values\n",
    "y = dataset.iloc[:, -4:].values\n",
    "print(\"Shape X \", x.shape)\n",
    "print(\"Shape Y \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x, test_x, train_y, test_y = train_test_split(x, y, test_size = 0.04)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import class_weight\n",
    "class_weights = class_weight.compute_class_weight('balanced',np.unique(df['type']),df['type'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape of X, Y train\", train_x.shape, train_y.shape)\n",
    "print(\"Shape of X, Y test\", test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_words = len(word_index)\n",
    "embedding_matrix = np.zeros((nb_words+1, 300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for word, i in word_index.items(): \n",
    "    if word in embeddings_index.keys():  \n",
    "        embedding_vector = embeddings_index[word] \n",
    "        #print(embedding_vector.shape)\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp1 = Input(shape = (30,)) \n",
    "\n",
    "x = Embedding(nb_words+1, embed_size, weights = [embedding_matrix])(inp1)\n",
    "\n",
    "\n",
    "x = Bidirectional(LSTM(10, return_sequences=False,dropout=0.1, recurrent_dropout=0.1))(x)\n",
    "\n",
    "x = Dense(32, activation=\"relu\")(x)\n",
    "x = Dropout(0.1)(x)\n",
    "x = Dense(4, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inp1, outputs=x)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\"weights.hdf5\", monitor='val_acc', verbose=0, save_best_only=True, mode='max')\n",
    "\n",
    "callbacks = [checkpoint]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs= 2\n",
    "\n",
    "y_test = [np.argmax(y) for y in test_y]\n",
    "\n",
    "for e in range(epochs):\n",
    "    model.fit(train_x, train_y, class_weight=class_weights,batch_size=32, epochs=1, nb_epoch=5, callbacks = callbacks, validation_split=0.1) #fit model\n",
    "\n",
    "    pred_glove_val_y = model.predict(test_x, batch_size=30, verbose=1) #make predictions\n",
    "\n",
    "    y_pred = [np.argmax(y) for y in pred_glove_val_y]\n",
    "    \n",
    "    score = f1_score(y_test, y_pred, average = 'micro')\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "            \n",
    "    print(\"Val F1 Score: {:.4f}\".format(score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def transformQ(query):\n",
    "  query = tokenizer.texts_to_sequences([query])\n",
    "  query = pad_sequences(query, maxlen)\n",
    "  return np.array(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dd={'query':[],'type': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "intent = ['cab','flight','hotel','weather']\n",
    "\n",
    "choice=input('Want to enter a query Y/N')\n",
    "while choice.lower() == 'y':\n",
    "  \n",
    "  print(\"cab=1; flight=3; hotel=3; weather=4\\n\")\n",
    "  query = input(\"Enter Query \")\n",
    "\n",
    "  q = transformQ(query)\n",
    "  \n",
    "  pred_glove_val_y = model.predict(q) \n",
    "\n",
    "  y_pred = np.argmax(pred_glove_val_y[0])\n",
    "  print(intent[y_pred],  pred_glove_val_y[0][y_pred], \"\\n\\n\")\n",
    "  \n",
    "  x=str(input('Did I predict correct y/n?'))\n",
    "  \n",
    "  if x.lower()=='n' or x.lower()=='no':\n",
    "    m=str(input('What type of query was it?'))\n",
    "    dd[\"query\"].append(query)\n",
    "    dd['type'].append(m)\n",
    "    \n",
    "  else:\n",
    "    pass \n",
    "  \n",
    "  \n",
    "  choice=input('Want to enter a query')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
